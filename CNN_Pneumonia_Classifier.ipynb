{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aramirezfr/Aircraft-Acquisition-Proposal/blob/master/CNN_Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct8_HIeB1rOa"
      },
      "source": [
        "**Pneumonia Image Binary Classifier Model** \\\n",
        "By: Adriana Ramirez Franco. \\\n",
        "Email: aramirezfr20@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RSuQMEQ1q3p"
      },
      "source": [
        "# Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi3Navfs1qOJ"
      },
      "source": [
        "**About Pneumonia:** \\\n",
        "Pneumonia is a serious respiratory condition that can lead to severe complications, especially if not diagnosed early. Timely and accurate diagnosis is crucial to initiate appropriate treatment and reduce morbidity and mortality, particularly among vulnerable populations like children, the elderly, and individuals with compromised immune systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ERY3aLUGqJa"
      },
      "source": [
        "**Why not using traditional in person diagnosis by doctors?** \\\n",
        "Traditional diagnosis of pneumonia relies heavily on radiologists interpreting chest X-rays, which can be time-consuming and prone to human error, especially under high workloads or in resource-limited settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c203yfPJGp-P"
      },
      "source": [
        "**How can we support a professional doctor's diagnosis?** \\\n",
        "To address these challenges, this project aims to develop a convolutional neural network (CNN)-based binary classification model to automatically identify pneumonia from chest X-ray images. By distinguishing between normal and pneumonia-affected lungs, the model will assist healthcare professionals in making quicker, more accurate decisions. This model can improve diagnostic efficiency, alleviate the burden on radiologists, enhance patient outcomes, and provide valuable support in remote or underserved areas where access to specialized radiology expertise is limited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTjdon_GyUA"
      },
      "source": [
        "## Benefits of implementing a Medical Image Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPul1f3KG-Ki"
      },
      "source": [
        "**Medical image classification using machine learning** is critically important for several reasons:  \n",
        "\n",
        "### **1. Faster Diagnosis and Treatment**  \n",
        "Convolutional neural networks (CNNs), can analyze medical images much faster than humans. This reduces the time required for diagnosis, enabling quicker initiation of treatment, which is particularly crucial for conditions like pneumonia, cancer, or strokes where delays can have life-threatening consequences.\n",
        "\n",
        "### **2. Improved Accuracy and Consistency**  \n",
        "Machine learning systems can match or exceed the diagnostic accuracy of radiologists in specific tasks, as they learn from large datasets and can identify patterns that may be difficult for human experts to detect. This ensures consistency in diagnosis, reducing human errors caused by fatigue or cognitive bias.\n",
        "\n",
        "### **3. Addressing Resource Gaps**  \n",
        "In many regions, especially remote or low-resource settings, there is a shortage of radiologists and specialized healthcare professionals. Machine learning models can act as decision-support tools to help non-specialists make informed diagnoses or prioritize cases that need expert attention.\n",
        "\n",
        "### **4. Reduced Workload for Healthcare Professionals**  \n",
        "With the increasing demand for medical imaging, radiologists often have to analyze hundreds of images per day. AI systems can help pre-screen images or highlight abnormal cases, allowing radiologists to focus their expertise on the most critical cases, improving workflow efficiency.\n",
        "\n",
        "### **5. Continuous Learning and Scalability**  \n",
        "Machine learning models can continuously improve as they are trained with new data, making them adaptable to emerging medical conditions. They are also scalable, meaning once a model is developed, it can be deployed across multiple healthcare systems globally with minimal modifications.\n",
        "\n",
        "### **6. Enabling Preventive Healthcare**  \n",
        "Automated image classification can also aid in early detection of diseases that may not exhibit symptoms initially, facilitating preventive interventions. For instance, AI models used for early screening of pneumonia or lung cancer can detect subtle abnormalities that might be missed in routine examinations.\n",
        "\n",
        "In summary, machine learning-based medical image classification is transforming healthcare by enhancing diagnostic accuracy, improving efficiency, and expanding access to quality care, making it an essential tool for modern medicine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2nJGrqy2R6m"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9s-F-3jHbBx"
      },
      "source": [
        "1. **Source and Properties of the Data**:\n",
        "   - This dataset, published by Paul Mooney on Kaggle, originates from the Guangzhou Women and Children’s Medical Center. It contains labeled chest X-ray images grouped into \"Pneumonia\" (with bacterial and viral categories) and \"Normal.\"\n",
        "   - The images are grayscale with consistent resolution, showing clear lung structures. The dataset is ideal for supervised machine learning tasks since each image is accurately labeled as pneumonia or healthy, making it a reliable choice for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Size of Data and Descriptive Statistics of Features**:\n",
        "   - The dataset comprises 5,863 images, divided into training, validation, and test sets, enabling efficient model evaluation. The training set includes around 4,000 images, with a smaller validation and test set.\n",
        "   - Key features include image pixel intensity values, which represent lung opacity patterns. The dataset contains approximately three times more pneumonia cases than normal ones, making it slightly imbalanced."
      ],
      "metadata": {
        "id": "v-4jY2Q9IZUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Feature Suitability**:\n",
        "   The primary feature—chest X-ray images—allows for visual detection of pneumonia markers, such as lung opacity and structure irregularities. This aligns well with the objective to classify cases of pneumonia versus healthy lungs based on these medical imaging patterns.\n"
      ],
      "metadata": {
        "id": "48uWpkSzIZ6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Limitations of Using This Data**:\n",
        "   - *Challenges*: The dataset's class imbalance (more pneumonia cases than normal) could affect model performance. The variation in image quality and possible label inconsistencies may also introduce noise, impacting model accuracy and generalizability.\n",
        "   - *Generalization Limits*: Since the dataset was sourced from a specific medical center, models trained on it might not generalize well to X-rays from different machines or patient demographics.\n"
      ],
      "metadata": {
        "id": "isee_AEtIZCA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nnzxVyW2gqX"
      },
      "source": [
        "For further details, refer to the Kaggle dataset page: [Chest X-Ray Images (Pneumonia) on Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5K5DjVDKBC2"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwRgCPN0RN-H",
        "outputId": "b337118f-6206-4c28-dd72-c2da64343d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "chest-xray-pneumonia.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "#Downloading the data file from Kaggle\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1PRxrvJz8UA",
        "outputId": "1eae9953-51d2-4842-fd2a-b888354d945e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  chest-xray-pneumonia.zip\n",
            "replace data/chest_xray/__MACOSX/._chest_xray? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#Unzip the data folder\n",
        "!unzip chest-xray-pneumonia.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "Yv2x1QL60T6R"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slQOxCnQJjZP"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A29NOR-v1fSN",
        "outputId": "5e4dde57-a800-4bf1-b4f4-69786a5d468b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'val', '__MACOSX', 'test', 'chest_xray']\n"
          ]
        }
      ],
      "source": [
        "#setting the chest_ray folder a directory\n",
        "directory=os.listdir('data/chest_xray')\n",
        "print(directory) #listing contents of directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "FrKjPIYj2WlP"
      },
      "outputs": [],
      "source": [
        "#directories inside the chest_xray file\n",
        "train_dir='data/chest_xray/train'\n",
        "val_dir='data/chest_xray/val'\n",
        "test_dir='data/chest_xray/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkLhv3dtKQgY"
      },
      "source": [
        "Using image_dataset_from directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "H9fS3_lrJ0-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5528fb-1d8b-4569-adde-9e56597e4148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Found 624 files belonging to 2 classes.\n",
            "Found 16 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#importing files\n",
        "train=keras.utils.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    image_size=(150, 150))\n",
        "\n",
        "test = keras.utils.image_dataset_from_directory (\n",
        "    directory = test_dir,\n",
        "    shuffle=True,\n",
        "    labels = \"inferred\",\n",
        "    label_mode='binary',\n",
        "    batch_size = 32,\n",
        "    image_size = (150,150))\n",
        "validation = keras.utils.image_dataset_from_directory (\n",
        "    directory = val_dir,\n",
        "    shuffle=True,\n",
        "    labels =\"inferred\",\n",
        "    label_mode='binary',\n",
        "    batch_size = 32,\n",
        "    image_size = (150 , 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpXUtiR3et1i",
        "outputId": "3b59bea3-1607-4790-9f68-a7f31d43f81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NORMAL', 'PNEUMONIA']\n",
            "['NORMAL', 'PNEUMONIA']\n",
            "['NORMAL', 'PNEUMONIA']\n"
          ]
        }
      ],
      "source": [
        "#checking the name of the classes in the files\n",
        "print(train.class_names)\n",
        "print(test.class_names)\n",
        "print(validation.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfQXiDbteyJw"
      },
      "source": [
        "Each file in 'chest_xray' includes a set of images with **\"Normal\"** x-rays and another set of x-rays with **\"Pneumonia\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axrau1Rle2Bc"
      },
      "source": [
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL0lz18ne1DA",
        "outputId": "69c21928-cb07-4e43-c27d-3facaf0f456b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5216"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "#define directories of training images\n",
        "pneumonia_dir = 'data/chest_xray/train/PNEUMONIA'\n",
        "normal_dir = 'data/chest_xray/train/NORMAL'\n",
        "\n",
        "#list files in each directory\n",
        "pneumonia_files = os.listdir(pneumonia_dir)\n",
        "normal_files = os.listdir(normal_dir)\n",
        "\n",
        "len(pneumonia_files)+len(normal_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzF2xZIge_7m"
      },
      "source": [
        "There are 5216 images to train on that belong to the subgroups 'PNEUMONIA' and 'NORMAL'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOIY0v9ifGjT"
      },
      "source": [
        "**Checking the images** in the files of the folder \"Training\" that contains the images of the x-rays that the model will train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anCnAcCrfCXJ"
      },
      "outputs": [],
      "source": [
        "#plot the Images\n",
        "def display_images(image_files, image_dir, num_images=5, title=''):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    for i, image_name in enumerate(image_files[:num_images]):\n",
        "        image_path = os.path.join(image_dir, image_name)\n",
        "        img = mpimg.imread(image_path)\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img, cmap='gray')  #using 'gray' for grayscale images\n",
        "        plt.title(image_name)\n",
        "        plt.axis('off')\n",
        "\n",
        "#display images from PNEUMONIA class\n",
        "display_images(pneumonia_files, pneumonia_dir, num_images=3, title='PNEUMONIA')\n",
        "\n",
        "#display images from NORMAL class\n",
        "display_images(normal_files, normal_dir, num_images=3, title='NORMAL')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the count of normal and x-rays with pneumonia\n",
        "len(pneumonia_files), len(normal_files)"
      ],
      "metadata": {
        "id": "tkSP8ywiU_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Out of the 100% (5,216) training images available, 74% of them are Pneumonia images and 26% of them are Normal Lung images. This demonstrates class imbalance. \\\n",
        "* In an ideally balanced dataset, each class would have an equal number of instances, typically 50% for each class for a binary classification problem. \\"
      ],
      "metadata": {
        "id": "rO_U4hYADMNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the count of pneumonia and normal class\n",
        "# create a variable with the counts:\n",
        "pneumonia_count = len(pneumonia_files)\n",
        "normal_count = len(normal_files)\n",
        "\n",
        "#define the labels and their corresponding counts\n",
        "labels = ['Pneumonia', 'Normal']\n",
        "counts = [pneumonia_count, normal_count]\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.bar(labels, counts) #labels on the x-axis and counts on the y-axis\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Imbalance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SD3Ww3AYU-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation:"
      ],
      "metadata": {
        "id": "yaXXD4pIubv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that there is a severe class imbalance in the training class I will use ImageDataGenerator to create some synthetic images to help balance the class and help prevent overfitting."
      ],
      "metadata": {
        "id": "lAABn71Xuf-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting parameters\n",
        "img_size=(100,100)\n",
        "SHAPE=(100,100,3)\n",
        "batch_size=32"
      ],
      "metadata": {
        "id": "nGwBTRsbnEqJ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "#data generator for train and validation\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                             rotation_range=20,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=False,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "#test data generator\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "metadata": {
        "id": "CSHMvV99-G02"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the data generator\n",
        "#training set\n",
        "train_set=datagen.flow_from_directory(train_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      #shuffle=False,\n",
        "                                      seed=42)\n",
        "\n",
        "val_set=datagen.flow_from_directory(val_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      seed=42)\n",
        "test_set=test_datagen.flow_from_directory(test_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      seed=42)"
      ],
      "metadata": {
        "id": "ShK1XLO1o2FF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80f8382-5d19-4e29-e74e-d5fc3a0b7d2e"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use train_set, val_set and test_set for next steps"
      ],
      "metadata": {
        "id": "7RLsvYKLFSep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ec67G4sgGqB"
      },
      "source": [
        "# Modeling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2334LNGgPSY"
      },
      "source": [
        "## Model Architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgCiAHOtgRai"
      },
      "source": [
        "## Base trial model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set metrics\n",
        "METRICS=['accuracy',\n",
        "         tf.keras.metrics.Precision(name='precision'),\n",
        "         tf.keras.metrics.Recall(name='recall')]"
      ],
      "metadata": {
        "id": "Pw_Orh6zLyKC"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "LH0GsELbgJ5J"
      },
      "outputs": [],
      "source": [
        "base_model = models.Sequential([\n",
        "\n",
        "    #first convolutional layer\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,100, 3)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "\n",
        "    #second convolutional layer\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "\n",
        "    #flatten the feature maps\n",
        "    layers.Flatten(),\n",
        "\n",
        "    #fully connected layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    #output layer with a single neuron for binary classification\n",
        "    layers.Dense(1, activation='sigmoid')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)"
      ],
      "metadata": {
        "id": "4dTE5SGTDgK0"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining callbacks for early stopping\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stopping=[EarlyStopping(monitor='val_acc', patience=10),\n",
        "                              ModelCheckpoint(filepath='best_model.keras',\n",
        "                              monitor='val_acc',\n",
        "                              save_best_only=True )]"
      ],
      "metadata": {
        "id": "9sXLgzmXMlxo"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiNT8WrwgZTj"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch=len(train)//batch_size\n",
        "validation_steps=len(test)//batch_size"
      ],
      "metadata": {
        "id": "aNC3W0CLON7u"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train base model\n",
        "base_model_hist = base_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)"
      ],
      "metadata": {
        "id": "Y1PjAFfaDp97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36af4cb-3c13-4e99-f292-249a25d8155b"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - acc: 0.5514 - loss: 127.4596 - precision: 0.5616 - recall: 0.5977 - val_acc: 0.5000 - val_loss: 24.1351 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 966ms/step - acc: 0.2745 - loss: 27.3569 - precision: 0.4136 - recall: 0.0572 - val_acc: 0.5000 - val_loss: 5.2679 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - acc: 0.8141 - loss: 1.9548 - precision: 0.8141 - recall: 1.0000 - val_acc: 0.5000 - val_loss: 3.6799 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 846ms/step - acc: 0.7052 - loss: 1.5911 - precision: 0.7122 - recall: 0.9863 - val_acc: 0.5000 - val_loss: 0.7162 - val_precision: 0.5000 - val_recall: 0.8750\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - acc: 0.6042 - loss: 0.6756 - precision: 0.7304 - recall: 0.7082 - val_acc: 0.4375 - val_loss: 0.7014 - val_precision: 0.4615 - val_recall: 0.7500\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - acc: 0.6121 - loss: 0.6724 - precision: 0.7401 - recall: 0.7476 - val_acc: 0.5000 - val_loss: 0.6875 - val_precision: 0.5000 - val_recall: 0.7500\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 860ms/step - acc: 0.7633 - loss: 0.6613 - precision: 0.7933 - recall: 0.9329 - val_acc: 0.5000 - val_loss: 0.6872 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 965ms/step - acc: 0.7063 - loss: 0.6440 - precision: 0.7201 - recall: 0.9510 - val_acc: 0.5000 - val_loss: 0.8640 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - acc: 0.6797 - loss: 0.7195 - precision: 0.6797 - recall: 1.0000 - val_acc: 0.5000 - val_loss: 0.6808 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 878ms/step - acc: 0.7050 - loss: 0.5452 - precision: 0.7050 - recall: 1.0000 - val_acc: 0.5000 - val_loss: 0.6816 - val_precision: 0.5000 - val_recall: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate base model\n",
        "results_base_train=base_model.evaluate(test_set)\n",
        "results_base_train"
      ],
      "metadata": {
        "id": "go8T4RQhDvkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcd97bc-45ee-4b57-d6a7-cd98b79414d4"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - acc: 0.3425 - loss: 0.7385 - precision: 0.3055 - recall: 0.6667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5973786115646362, 0.6426281929016113, 0.636215329170227, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basetest_loss, basetest_accuracy, basetest_precision, basetest_recall = results_base_train\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test accuracy: {basetest_accuracy:.2f}\")\n",
        "print(f\"Test precision: {basetest_precision:.2f}\")\n",
        "print(f\"Test recall: {basetest_recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFIyQevoZHYB",
        "outputId": "0f4cf88f-5623-45f6-b392-265b97d880da"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.64\n",
            "Test precision: 0.64\n",
            "Test recall: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model 1:"
      ],
      "metadata": {
        "id": "Zmz-M01ORz46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding a third layer and dropout layer to compare results with base model\n",
        "test1_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    #extra third layer\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    #adding dropout layer\n",
        "    Dropout(0.5),  #to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')  #output layer for binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "rDyeSwPAR_gi"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "test1_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)"
      ],
      "metadata": {
        "id": "mkIoNBFhZzUt"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the test1_model\n",
        "test1_history = test1_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHU7RbouZ2c4",
        "outputId": "ba4118b4-d861-45a5-da33-1386fd431569"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - acc: 0.4483 - loss: 20.7239 - precision: 0.7505 - recall: 0.9885 - val_acc: 0.4375 - val_loss: 1.3034 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 958ms/step - acc: 0.6010 - loss: 1.3641 - precision: 0.7802 - recall: 0.6590 - val_acc: 0.5000 - val_loss: 1.3237 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - acc: 0.6411 - loss: 0.8557 - precision: 0.7003 - recall: 0.8549 - val_acc: 0.5000 - val_loss: 0.8512 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 956ms/step - acc: 0.7620 - loss: 0.5804 - precision: 0.7884 - recall: 0.9577 - val_acc: 0.5000 - val_loss: 0.8987 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - acc: 0.7698 - loss: 0.5244 - precision: 0.8142 - recall: 0.9238 - val_acc: 0.5625 - val_loss: 0.7109 - val_precision: 0.5333 - val_recall: 1.0000\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 973ms/step - acc: 0.7661 - loss: 0.5908 - precision: 0.8144 - recall: 0.9108 - val_acc: 0.5000 - val_loss: 0.7174 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 941ms/step - acc: 0.7023 - loss: 0.5551 - precision: 0.7825 - recall: 0.8670 - val_acc: 0.5000 - val_loss: 0.7692 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - acc: 0.7554 - loss: 0.5391 - precision: 0.7572 - recall: 0.9853 - val_acc: 0.5000 - val_loss: 0.8842 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 936ms/step - acc: 0.7827 - loss: 0.5038 - precision: 0.7806 - recall: 1.0000 - val_acc: 0.5625 - val_loss: 0.8077 - val_precision: 0.5333 - val_recall: 1.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 983ms/step - acc: 0.7785 - loss: 0.5035 - precision: 0.8004 - recall: 0.9464 - val_acc: 0.5625 - val_loss: 0.7381 - val_precision: 0.5385 - val_recall: 0.8750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1_results= test1_model.evaluate(test_set)\n",
        "print(\"Evaluation results:\", test1_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oO9R3GvYX4t",
        "outputId": "89241449-c3a6-4e39-9fb3-24b44ebf8ab2"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 419ms/step - acc: 0.6659 - loss: 0.5974 - precision: 0.3896 - recall: 0.6407\n",
            "Evaluation results: [0.47300392389297485, 0.7820512652397156, 0.7570850253105164, 0.9589743614196777]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1_loss, test1_accuracy, test1_precision, test1_recall = test1_results\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test accuracy: {test1_accuracy:.2f}\")\n",
        "print(f\"Test precision: {test1_precision:.2f}\")\n",
        "print(f\"Test recall: {test1_recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llNFirPpY3m2",
        "outputId": "c678710e-535a-42cb-fc09-6abc2a691856"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.78\n",
            "Test precision: 0.76\n",
            "Test recall: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training history\n",
        "plt.plot(test1_history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(test1_history.history['val_accuracy'], label='val accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4BNe91pOZ67S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "500960f5-cc7f-4242-d519-b8b9a1ba26d7"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'accuracy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-0de868a39302>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot the training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest1_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest1_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WujGguaPgNlb"
      },
      "source": [
        "MAIN MODEL TRIALLLLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "87_FJSlXgNK_"
      },
      "outputs": [],
      "source": [
        "test2_model = models.Sequential([\n",
        "    #input layer(32 filters, 3x3 kernel size, \"relu\" activation,input shape 150x150 to fit image generator function)\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "\n",
        "    #layer with 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    #64 filters\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(), #flatten layer\n",
        "\n",
        "    #Dense layer with 512 neurons\n",
        "    layers.Dense(512, activation='relu'),#this dense layer input matches the flattened output\n",
        "\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    #output layer: dense layer with 2 layers\n",
        "    layers.Dense(1, activation='sigmoid')  #for binary classification\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "pZZFy2CFD6xd"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "0LDlSmO2gx-a",
        "outputId": "97c81a29-731c-40d8-eee6-65343c655d03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_40 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_40 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_41 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_42 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_43 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_15 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m524,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_16               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │           \u001b[38;5;34m1,026\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_16               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_17               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_19               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,415,234\u001b[0m (5.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,415,234</span> (5.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,411,138\u001b[0m (5.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,411,138</span> (5.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,096\u001b[0m (16.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> (16.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#print summary model of architecture\n",
        "test2_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "test2_history = test2_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)\n"
      ],
      "metadata": {
        "id": "mfhR82Jta6Ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334fb051-3f79-4c7b-d29f-c5572b0782a1"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5591 - loss: 0.7713 - precision: 0.8168 - recall: 0.5310 - val_accuracy: 0.4375 - val_loss: 1.4025 - val_precision: 0.4667 - val_recall: 0.8750\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5247 - loss: 0.9423 - precision: 0.7613 - recall: 0.5282 - val_accuracy: 0.5000 - val_loss: 1.7096 - val_precision: 0.5000 - val_recall: 0.8750\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 956ms/step - accuracy: 0.6694 - loss: 0.7380 - precision: 0.8586 - recall: 0.6688 - val_accuracy: 0.5000 - val_loss: 3.9945 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 939ms/step - accuracy: 0.6852 - loss: 0.7203 - precision: 0.8152 - recall: 0.7073 - val_accuracy: 0.5000 - val_loss: 4.2820 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6717 - loss: 0.6502 - precision: 0.8359 - recall: 0.7134 - val_accuracy: 0.5625 - val_loss: 4.0022 - val_precision: 0.5333 - val_recall: 1.0000\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.7904 - loss: 0.4997 - precision: 0.8937 - recall: 0.8087 - val_accuracy: 0.5000 - val_loss: 6.5899 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.7386 - loss: 0.5440 - precision: 0.8440 - recall: 0.7919 - val_accuracy: 0.5625 - val_loss: 0.9590 - val_precision: 0.5714 - val_recall: 0.5000\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.8196 - loss: 0.4100 - precision: 0.9259 - recall: 0.8274 - val_accuracy: 0.4375 - val_loss: 2.8075 - val_precision: 0.4667 - val_recall: 0.8750\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.8589 - loss: 0.3555 - precision: 0.9291 - recall: 0.8875 - val_accuracy: 0.5625 - val_loss: 6.2869 - val_precision: 0.5333 - val_recall: 1.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 941ms/step - accuracy: 0.7929 - loss: 0.4009 - precision: 0.8473 - recall: 0.8678 - val_accuracy: 0.6250 - val_loss: 0.7887 - val_precision: 0.6667 - val_recall: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_results= test2_model.evaluate(test_set)\n",
        "print(\"Evaluation results:\", test2_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuOWZAW7bnJp",
        "outputId": "048de7e7-4a62-4bab-c8ff-43ef0e522c71"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 417ms/step - accuracy: 0.7686 - loss: 0.4866 - precision: 0.4627 - recall: 0.4725\n",
            "Evaluation results: [0.5687832832336426, 0.7211538553237915, 0.8396226167678833, 0.6846153736114502]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_loss, test2_accuracy, test2_precision, test2_recall = test2_results\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test accuracy: {test2_accuracy:.2f}\")\n",
        "print(f\"Test precision: {test2_precision:.2f}\")\n",
        "print(f\"Test recall: {test2_recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2J0cxtscP-F",
        "outputId": "53cab501-ed04-443d-e348-612f045086e2"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.72\n",
            "Test precision: 0.84\n",
            "Test recall: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcRtgV3BcUJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDV-k5tlg0qC"
      },
      "source": [
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuVitwhAhDKi"
      },
      "source": [
        "## Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yOMBD5DhhBQw"
      },
      "outputs": [],
      "source": [
        "history= model.fit(train,\n",
        "                        epochs=10,\n",
        "                        validation_data=test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRYPiGshWC0"
      },
      "source": [
        "# Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking accuracy:\n",
        "results_train= model.evaluate(train, batch_size=128)\n",
        "results_train"
      ],
      "metadata": {
        "id": "MFjm3jkYEHCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_test= model.evaluate(test, batch_size=128)\n",
        "results_test"
      ],
      "metadata": {
        "id": "rg0NweklEG40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(10,10))"
      ],
      "metadata": {
        "id": "71YxKdOlEGuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl74s7VJhXzI"
      },
      "outputs": [],
      "source": [
        "#evaluating with testing image\n",
        "#Evaluation of normal image\n",
        "#using normal x-ray image\n",
        "#test_image=tf.keras.utils.load_img('data/chest_xray/test/NORMAL/IM-0013-0001.jpeg',\n",
        "                                   target_size=(150,150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwckVU3sheFn"
      },
      "outputs": [],
      "source": [
        "#display test image\n",
        "#plt.imshow(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VaVLEc6hm6G"
      },
      "outputs": [],
      "source": [
        "#expand dimensions to match expected input shape\n",
        "#test_image=tf.keras.utils.img_to_array(test_image)\n",
        "#test_image=np.expand_dims(test_image, axis=0)\n",
        "#used trained model to make a prediction\n",
        "#result=model.predict(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOy35LvPhpdH"
      },
      "outputs": [],
      "source": [
        "#print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiTXJj2Lh0J4"
      },
      "source": [
        "\n",
        "Create a confusion Matrix to analyze the description of the performace of the classification model on the set of test data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm= confusion_matrix(y_true=, y_pred=result)"
      ],
      "metadata": {
        "id": "PeQsedELH12y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Create a classification report."
      ],
      "metadata": {
        "id": "W9sp_OAANR0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred=model.predict(test)\n",
        "y_pred_classes = np.round(y_pred).astype(int)\n",
        "#generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6RZ0HaUqNTGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a classification report.\n",
        "report = classification_report(y_test, y_pred_classes)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "Ke8oo_QsEQe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:"
      ],
      "metadata": {
        "id": "5AvEQNh5Jra7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results:"
      ],
      "metadata": {
        "id": "_BzyGPIvN_cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training process(epochs?)\\\n",
        "how long does each epoch takes?\\\n",
        "total training time?\\\n",
        "results? \\\n",
        "\n",
        "true positives!!\\\n",
        "true negatives!!\\\n",
        "false positives??\\\n",
        "false negatives X (minimize) \\"
      ],
      "metadata": {
        "id": "o6e1KF0TOFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67-K95KFN94P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5RSuQMEQ1q3p",
        "W2nJGrqy2R6m",
        "3Ec67G4sgGqB",
        "4jRYPiGshWC0"
      ],
      "authorship_tag": "ABX9TyOFR2RNLU/uXcSgnJC3ey9p",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}