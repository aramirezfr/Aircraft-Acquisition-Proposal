{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aramirezfr/Aircraft-Acquisition-Proposal/blob/master/CNN_Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct8_HIeB1rOa"
      },
      "source": [
        "**Pneumonia Image Binary Classifier Model** \\\n",
        "By: Adriana Ramirez Franco. \\\n",
        "Email: aramirezfr20@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RSuQMEQ1q3p"
      },
      "source": [
        "# Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi3Navfs1qOJ"
      },
      "source": [
        "**About Pneumonia:** \\\n",
        "Pneumonia is a serious respiratory condition that can lead to severe complications, especially if not diagnosed early. Timely and accurate diagnosis is crucial to initiate appropriate treatment and reduce morbidity and mortality, particularly among vulnerable populations like children, the elderly, and individuals with compromised immune systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ERY3aLUGqJa"
      },
      "source": [
        "**Why not using traditional in person diagnosis by doctors?** \\\n",
        "Traditional diagnosis of pneumonia relies heavily on radiologists interpreting chest X-rays, which can be time-consuming and prone to human error, especially under high workloads or in resource-limited settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c203yfPJGp-P"
      },
      "source": [
        "**How can we support a professional doctor's diagnosis?** \\\n",
        "To address these challenges, this project aims to develop a convolutional neural network (CNN)-based binary classification model to automatically identify pneumonia from chest X-ray images. By distinguishing between normal and pneumonia-affected lungs, the model will assist healthcare professionals in making quicker, more accurate decisions. This model can improve diagnostic efficiency, alleviate the burden on radiologists, enhance patient outcomes, and provide valuable support in remote or underserved areas where access to specialized radiology expertise is limited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTjdon_GyUA"
      },
      "source": [
        "## Benefits of implementing a Medical Image Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPul1f3KG-Ki"
      },
      "source": [
        "**Medical image classification using machine learning** is critically important for several reasons:  \n",
        "\n",
        "### **1. Faster Diagnosis and Treatment**  \n",
        "Convolutional neural networks (CNNs), can analyze medical images much faster than humans. This reduces the time required for diagnosis, enabling quicker initiation of treatment, which is particularly crucial for conditions like pneumonia, cancer, or strokes where delays can have life-threatening consequences.\n",
        "\n",
        "### **2. Improved Accuracy and Consistency**  \n",
        "Machine learning systems can match or exceed the diagnostic accuracy of radiologists in specific tasks, as they learn from large datasets and can identify patterns that may be difficult for human experts to detect. This ensures consistency in diagnosis, reducing human errors caused by fatigue or cognitive bias.\n",
        "\n",
        "### **3. Addressing Resource Gaps**  \n",
        "In many regions, especially remote or low-resource settings, there is a shortage of radiologists and specialized healthcare professionals. Machine learning models can act as decision-support tools to help non-specialists make informed diagnoses or prioritize cases that need expert attention.\n",
        "\n",
        "### **4. Reduced Workload for Healthcare Professionals**  \n",
        "With the increasing demand for medical imaging, radiologists often have to analyze hundreds of images per day. AI systems can help pre-screen images or highlight abnormal cases, allowing radiologists to focus their expertise on the most critical cases, improving workflow efficiency.\n",
        "\n",
        "### **5. Continuous Learning and Scalability**  \n",
        "Machine learning models can continuously improve as they are trained with new data, making them adaptable to emerging medical conditions. They are also scalable, meaning once a model is developed, it can be deployed across multiple healthcare systems globally with minimal modifications.\n",
        "\n",
        "### **6. Enabling Preventive Healthcare**  \n",
        "Automated image classification can also aid in early detection of diseases that may not exhibit symptoms initially, facilitating preventive interventions. For instance, AI models used for early screening of pneumonia or lung cancer can detect subtle abnormalities that might be missed in routine examinations.\n",
        "\n",
        "In summary, machine learning-based medical image classification is transforming healthcare by enhancing diagnostic accuracy, improving efficiency, and expanding access to quality care, making it an essential tool for modern medicine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2nJGrqy2R6m"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9s-F-3jHbBx"
      },
      "source": [
        "1. **Source and Properties of the Data**:\n",
        "   - This dataset, published by Paul Mooney on Kaggle, originates from the Guangzhou Women and Children’s Medical Center. It contains labeled chest X-ray images grouped into \"Pneumonia\" (with bacterial and viral categories) and \"Normal.\"\n",
        "   - The images are grayscale with consistent resolution, showing clear lung structures. The dataset is ideal for supervised machine learning tasks since each image is accurately labeled as pneumonia or healthy, making it a reliable choice for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Size of Data and Descriptive Statistics of Features**:\n",
        "   - The dataset comprises 5,863 images, divided into training, validation, and test sets, enabling efficient model evaluation. The training set includes around 4,000 images, with a smaller validation and test set.\n",
        "   - Key features include image pixel intensity values, which represent lung opacity patterns. The dataset contains approximately three times more pneumonia cases than normal ones, making it slightly imbalanced."
      ],
      "metadata": {
        "id": "v-4jY2Q9IZUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Feature Suitability**:\n",
        "   The primary feature—chest X-ray images—allows for visual detection of pneumonia markers, such as lung opacity and structure irregularities. This aligns well with the objective to classify cases of pneumonia versus healthy lungs based on these medical imaging patterns.\n"
      ],
      "metadata": {
        "id": "48uWpkSzIZ6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Limitations of Using This Data**:\n",
        "   - *Challenges*: The dataset's class imbalance (more pneumonia cases than normal) could affect model performance. The variation in image quality and possible label inconsistencies may also introduce noise, impacting model accuracy and generalizability.\n",
        "   - *Generalization Limits*: Since the dataset was sourced from a specific medical center, models trained on it might not generalize well to X-rays from different machines or patient demographics.\n"
      ],
      "metadata": {
        "id": "isee_AEtIZCA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nnzxVyW2gqX"
      },
      "source": [
        "For further details, refer to the Kaggle dataset page: [Chest X-Ray Images (Pneumonia) on Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5K5DjVDKBC2"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will begin downloading the necessary file from Kaggle, unzip the file, and I will be importing the necessary libraries for this project."
      ],
      "metadata": {
        "id": "zMcSmaIbjwri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwRgCPN0RN-H"
      },
      "outputs": [],
      "source": [
        "#Downloading the data file from Kaggle\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1PRxrvJz8UA"
      },
      "outputs": [],
      "source": [
        "#Unzip the data folder\n",
        "!unzip chest-xray-pneumonia.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv2x1QL60T6R"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slQOxCnQJjZP"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A29NOR-v1fSN"
      },
      "outputs": [],
      "source": [
        "#setting the directory that contains the kaggle data file\n",
        "directory=os.listdir('data/chest_xray')\n",
        "print(directory) #listing contents of directory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 directories inside the chest_xray file: training, validation and testing directory."
      ],
      "metadata": {
        "id": "0dSQjJYQk0BV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrKjPIYj2WlP"
      },
      "outputs": [],
      "source": [
        "#setting variables to set the directories\n",
        "train_dir='data/chest_xray/train'\n",
        "val_dir='data/chest_xray/val'\n",
        "test_dir='data/chest_xray/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkLhv3dtKQgY"
      },
      "source": [
        "Using **image_dataset_from_directory** function to create a dataset from images stored in the previous directories.\n",
        "I will create 3 different datasets, one for each directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9fS3_lrJ0-n"
      },
      "outputs": [],
      "source": [
        "#creating datasets for all directories\n",
        "train=keras.utils.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    image_size=(150, 150))\n",
        "\n",
        "test = keras.utils.image_dataset_from_directory (\n",
        "    directory = test_dir,\n",
        "    shuffle=True,\n",
        "    labels = \"inferred\",\n",
        "    label_mode='binary',\n",
        "    batch_size = 32,\n",
        "    image_size = (150,150))\n",
        "validation = keras.utils.image_dataset_from_directory (\n",
        "    directory = val_dir,\n",
        "    shuffle=True,\n",
        "    labels =\"inferred\",\n",
        "    label_mode='binary',\n",
        "    batch_size = 32,\n",
        "    image_size = (150 , 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpXUtiR3et1i"
      },
      "outputs": [],
      "source": [
        "#checking the name of the classes in the files\n",
        "print(train.class_names)\n",
        "print(test.class_names)\n",
        "print(validation.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfQXiDbteyJw"
      },
      "source": [
        "Each file in 'chest_xray' includes a set of images with **\"Normal\"** x-rays and another set of x-rays with **\"Pneumonia\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axrau1Rle2Bc"
      },
      "source": [
        "------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training data:**\n",
        "\n",
        "The model uses training data to learn patterns, relationships, and rules that map inputs to outputs. A well-trained model should generalize from the training data to make accurate predictions on new, unseen data. The quality and diversity of the training data significantly affect the model's ability to generalize.A model trained on imbalanced data may not generalize well to real-world scenarios where the distribution of classes might be different. This can lead to poor performance in practical applications."
      ],
      "metadata": {
        "id": "A0fyT78uoPeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL0lz18ne1DA"
      },
      "outputs": [],
      "source": [
        "#defineing directories of training images\n",
        "pneumonia_dir = 'data/chest_xray/train/PNEUMONIA'\n",
        "normal_dir = 'data/chest_xray/train/NORMAL'\n",
        "\n",
        "#list files in each directory\n",
        "pneumonia_files = os.listdir(pneumonia_dir)\n",
        "normal_files = os.listdir(normal_dir)\n",
        "\n",
        "#checking the quantity of images in each directory\n",
        "len(pneumonia_files)+len(normal_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzF2xZIge_7m"
      },
      "source": [
        "There are 5216 images to train on that belong to the subgroups **'PNEUMONIA'** and **'NORMAL'**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "OPSZ0Pd3p-_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a look to see the difference between X-rays that show lungs with pneumonia and normal healthy lungs."
      ],
      "metadata": {
        "id": "qk84Z9SlrnE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anCnAcCrfCXJ"
      },
      "outputs": [],
      "source": [
        "#plot the count of images\n",
        "def display_images(image_files, image_dir, num_images=5, title=''):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    for i, image_name in enumerate(image_files[:num_images]):\n",
        "        image_path = os.path.join(image_dir, image_name)\n",
        "        img = mpimg.imread(image_path)\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img, cmap='gray')  #using 'gray' for grayscale images\n",
        "        plt.title(image_name)\n",
        "        plt.axis('off')\n",
        "\n",
        "#display images from PNEUMONIA class\n",
        "display_images(pneumonia_files, pneumonia_dir, num_images=3, title='PNEUMONIA')\n",
        "\n",
        "#display images from NORMAL class\n",
        "display_images(normal_files, normal_dir, num_images=3, title='NORMAL')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Out of the 100% (5,216) training images available, 74% of them are Pneumonia images and 26% of them are Normal Lung images. This demonstrates class imbalance. \\\n",
        "* In an ideally balanced dataset, each class would have an equal number of instances, typically approximately around 50% for each class for a binary classification problem. \\"
      ],
      "metadata": {
        "id": "rO_U4hYADMNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the count of normal and x-rays with pneumonia\n",
        "len(pneumonia_files), len(normal_files)"
      ],
      "metadata": {
        "id": "tkSP8ywiU_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOIY0v9ifGjT"
      },
      "source": [
        "Plot the count of the images in the Training file to check for imbalance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the count of pneumonia and normal class\n",
        "# create a variable with the counts:\n",
        "pneumonia_count = len(pneumonia_files)\n",
        "normal_count = len(normal_files)\n",
        "\n",
        "#define the labels and their corresponding counts\n",
        "labels = ['Pneumonia', 'Normal']\n",
        "counts = [pneumonia_count, normal_count]\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.bar(labels, counts) #labels on the x-axis and counts on the y-axis\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Imbalance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SD3Ww3AYU-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation:"
      ],
      "metadata": {
        "id": "yaXXD4pIubv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that there is a severe class imbalance in the training class I will use ImageDataGenerator to create some synthetic images to help balance the class and help prevent overfitting."
      ],
      "metadata": {
        "id": "lAABn71Xuf-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting parameters\n",
        "img_size=(100,100)\n",
        "SHAPE=(100,100,3)\n",
        "batch_size=32"
      ],
      "metadata": {
        "id": "nGwBTRsbnEqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the data generator for train and validation\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                             rotation_range=20,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=False,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "#setting the test data generator\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "metadata": {
        "id": "CSHMvV99-G02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the data generator\n",
        "#training and validation set\n",
        "train_set=datagen.flow_from_directory(train_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      #shuffle=False,\n",
        "                                      seed=42)\n",
        "\n",
        "val_set=datagen.flow_from_directory(val_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      seed=42)\n",
        "test_set=test_datagen.flow_from_directory(test_dir,\n",
        "                                      class_mode='binary',\n",
        "                                      target_size=img_size,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      seed=42)"
      ],
      "metadata": {
        "id": "ShK1XLO1o2FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the directories will be going through **ImageDataGenerator** their names (train_dir, val_dir, test_dir) will change to: train_set, val_set and test_set."
      ],
      "metadata": {
        "id": "taqKdIrQs-IB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ec67G4sgGqB"
      },
      "source": [
        "# Modeling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2334LNGgPSY"
      },
      "source": [
        "**Model building** is an iterative process that starts from a baseline model to more complex models based on rationales that will be redefined on each iteration.\\\n",
        "I will start with a baseline model that will be used for comparison.\n",
        "New model iterations will be justified on the models that will come after the base model.\n",
        "New models will show a different result and we are looking for the best model that will give us the best result for this binary classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgCiAHOtgRai"
      },
      "source": [
        "## Baseline model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting metrics for all the models to be trained\n",
        "METRICS=['accuracy',\n",
        "         tf.keras.metrics.Precision(name='precision'),\n",
        "         tf.keras.metrics.Recall(name='recall')]"
      ],
      "metadata": {
        "id": "Pw_Orh6zLyKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH0GsELbgJ5J"
      },
      "outputs": [],
      "source": [
        "#setting up the baseline model\n",
        "base_model = models.Sequential([\n",
        "\n",
        "    #first convolutional layer\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,100, 3)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "\n",
        "    #second convolutional layer\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "\n",
        "    #flatten the feature maps\n",
        "    layers.Flatten(),\n",
        "\n",
        "    #fully connected layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    #output layer with a single neuron for binary classification\n",
        "    layers.Dense(1, activation='sigmoid')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)"
      ],
      "metadata": {
        "id": "4dTE5SGTDgK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping:**\n",
        "\n",
        "Is used to prevent overfitting and improve the training process. Early stopping can reduce the computational cost by stopping the training process once the model stops improving, saving time and computational resources."
      ],
      "metadata": {
        "id": "-1Dx4i341sRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining callbacks for early stopping\n",
        "early_stopping=[EarlyStopping(monitor='val_acc', patience=10),\n",
        "                              ModelCheckpoint(filepath='best_model.keras',\n",
        "                              monitor='val_acc',\n",
        "                              save_best_only=True )]"
      ],
      "metadata": {
        "id": "9sXLgzmXMlxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiNT8WrwgZTj"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculates # of batches needed to cover the training set in one epoch\n",
        "steps_per_epoch=len(train_set)//batch_size\n",
        "\n",
        "#calculates # of batches needed to cover the validation set\n",
        "validation_steps=len(val_set)//batch_size"
      ],
      "metadata": {
        "id": "j7Y7cf1K71xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculates # of batches needed to cover the training set in one epoch\n",
        "steps_per_epoch=len(train)//batch_size\n",
        "\n",
        "#calculates # of batches needed to cover the validation set\n",
        "validation_steps=len(val)//batch_size"
      ],
      "metadata": {
        "id": "aNC3W0CLON7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train base model\n",
        "base_model_hist = base_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)"
      ],
      "metadata": {
        "id": "Y1PjAFfaDp97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate base model\n",
        "results_base_train=base_model.evaluate(test_set)\n",
        "results_base_train"
      ],
      "metadata": {
        "id": "go8T4RQhDvkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unpacking values\n",
        "basetest_loss, basetest_accuracy, basetest_precision, basetest_recall = results_base_train\n",
        "\n",
        "#print the result values\n",
        "print(f\"Test accuracy: {basetest_accuracy:.2f}\")\n",
        "print(f\"Test precision: {basetest_precision:.2f}\")\n",
        "print(f\"Test recall: {basetest_recall:.2f}\")"
      ],
      "metadata": {
        "id": "zFIyQevoZHYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's why it's important and how focusing on precision and recall can benefit you:\n",
        "\n",
        "**Visual Diagnosis:**\n",
        "Detecting Overfitting/Underfitting: By plotting these metrics, you can visually assess whether your model is overfitting or underfitting. Overfitting is indicated by a large gap between training and validation metrics, where training performance is much better than validation performance. Underfitting might be suggested if both metrics are poor.\\\n",
        "**Training Stability:** You can see if the training process is stable or if the model's performance is volatile, which might suggest problems with the learning rate or data quality.\n",
        "\n",
        "**Focusing on Precision and Recall:**\n",
        "Precision: Important when the cost of false positives is high. A precision plot can help you understand how well the model is maintaining this aspect during training and validation.\n",
        "Recall: Crucial when missing positive instances is costly. Plotting recall helps ensure the model is effectively identifying positive cases throughout training.\n",
        "\n",
        "**Hyperparameter Tuning:** By visualizing these metrics, you can better understand how different hyperparameters (e.g., batch size, learning rate) affect model performance. This facilitates more informed decisions during hyperparameter tuning."
      ],
      "metadata": {
        "id": "wI1_XVwJ-kqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot precision and recall\n",
        "#(train)reflect how well the model is learning the patterns in the training data\n",
        "plt.plot(base_model_hist.history['precision'], label='Train Precision')\n",
        "\n",
        "#(validation)model's performance on unseen data\n",
        "plt.plot(base_model_hist.history['val_precision'], label='Validation Precision')\n",
        "plt.plot(base_model_hist.history['recall'], label='Train Recall')\n",
        "plt.plot(base_model_hist.history['val_recall'], label='Validation Recall')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision and Recall over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z8mQCHKv_0H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training history\n",
        "plt.plot(base_model_hist.history['accuracy'], label='train accuracy')\n",
        "plt.plot(base_model_hist.history['val_accuracy'], label='val accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFW1PlcfebRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model 1:"
      ],
      "metadata": {
        "id": "Zmz-M01ORz46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding a third layer and dropout layer to compare results with base model\n",
        "test1_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    #extra third layer\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    #adding dropout layer\n",
        "    Dropout(0.5),  #to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')  #output layer for binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "rDyeSwPAR_gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "test1_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)"
      ],
      "metadata": {
        "id": "mkIoNBFhZzUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the test1_model\n",
        "test1_history = test1_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)"
      ],
      "metadata": {
        "id": "mHU7RbouZ2c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1_results= test1_model.evaluate(test_set)\n",
        "print(\"Evaluation results:\", test1_results)"
      ],
      "metadata": {
        "id": "5oO9R3GvYX4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1_loss, test1_accuracy, test1_precision, test1_recall = test1_results\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test accuracy: {test1_accuracy:.2f}\")\n",
        "print(f\"Test precision: {test1_precision:.2f}\")\n",
        "print(f\"Test recall: {test1_recall:.2f}\")"
      ],
      "metadata": {
        "id": "llNFirPpY3m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training history\n",
        "plt.plot(test1_history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(test1_history.history['val_accuracy'], label='val accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4BNe91pOZ67S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WujGguaPgNlb"
      },
      "source": [
        "MAIN MODEL TRIALLLLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87_FJSlXgNK_"
      },
      "outputs": [],
      "source": [
        "test2_model = models.Sequential([\n",
        "    #input layer(32 filters, 3x3 kernel size, \"relu\" activation,input shape 150x150 to fit image generator function)\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "\n",
        "    #layer with 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    #64 filters\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(), #flatten layer\n",
        "\n",
        "    #Dense layer with 512 neurons\n",
        "    layers.Dense(512, activation='relu'),#this dense layer input matches the flattened output\n",
        "\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    #output layer: dense layer with 2 layers\n",
        "    layers.Dense(1, activation='sigmoid')  #for binary classification\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "pZZFy2CFD6xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LDlSmO2gx-a"
      },
      "outputs": [],
      "source": [
        "#print summary model of architecture\n",
        "test2_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "test2_history = test2_model.fit(train_set,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 epochs=10,\n",
        "                                 callbacks=early_stopping,\n",
        "                                 validation_data=val_set,\n",
        "                                 shuffle=False)\n"
      ],
      "metadata": {
        "id": "mfhR82Jta6Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2_results= test2_model.evaluate(test_set)\n",
        "print(\"Evaluation results:\", test2_results)"
      ],
      "metadata": {
        "id": "AuOWZAW7bnJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2_loss, test2_accuracy, test2_precision, test2_recall = test2_results\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test accuracy: {test2_accuracy:.2f}\")\n",
        "print(f\"Test precision: {test2_precision:.2f}\")\n",
        "print(f\"Test recall: {test2_recall:.2f}\")"
      ],
      "metadata": {
        "id": "D2J0cxtscP-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training history\n",
        "plt.plot(test2_history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(test2_history.history['val_accuracy'], label='val accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BcRtgV3BcUJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDV-k5tlg0qC"
      },
      "source": [
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuVitwhAhDKi"
      },
      "source": [
        "## Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yOMBD5DhhBQw"
      },
      "outputs": [],
      "source": [
        "history= model.fit(train,\n",
        "                        epochs=10,\n",
        "                        validation_data=test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRYPiGshWC0"
      },
      "source": [
        "# Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking accuracy:\n",
        "results_train= model.evaluate(train, batch_size=128)\n",
        "results_train"
      ],
      "metadata": {
        "id": "MFjm3jkYEHCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_test= model.evaluate(test, batch_size=128)\n",
        "results_test"
      ],
      "metadata": {
        "id": "rg0NweklEG40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(10,10))"
      ],
      "metadata": {
        "id": "71YxKdOlEGuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiTXJj2Lh0J4"
      },
      "source": [
        "\n",
        "Create a confusion Matrix to analyze the description of the performace of the classification model on the set of test data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm= confusion_matrix(y_true=, y_pred=result)"
      ],
      "metadata": {
        "id": "PeQsedELH12y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Create a classification report."
      ],
      "metadata": {
        "id": "W9sp_OAANR0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred=model.predict(test)\n",
        "y_pred_classes = np.round(y_pred).astype(int)\n",
        "#generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6RZ0HaUqNTGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a classification report.\n",
        "report = classification_report(y_test, y_pred_classes)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "Ke8oo_QsEQe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:"
      ],
      "metadata": {
        "id": "5AvEQNh5Jra7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results:"
      ],
      "metadata": {
        "id": "_BzyGPIvN_cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training process(epochs?)\\\n",
        "how long does each epoch takes?\\\n",
        "total training time?\\\n",
        "results? \\\n",
        "\n",
        "true positives!!\\\n",
        "true negatives!!\\\n",
        "false positives??\\\n",
        "false negatives X (minimize) \\"
      ],
      "metadata": {
        "id": "o6e1KF0TOFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67-K95KFN94P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5RSuQMEQ1q3p",
        "W2nJGrqy2R6m",
        "3Ec67G4sgGqB",
        "4jRYPiGshWC0"
      ],
      "authorship_tag": "ABX9TyMlHIfcTb4rAAGtCvMb7ubl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}